{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NY taxi trip data is already clean. It includes January 2019.\n",
    "\n",
    "This is my first neural network model so I am using extensivly code from Geron's \"Hands on machine learning\".\n",
    "\n",
    "The task is to check if sequential MLP model will does better then Random Forest model I done before in other file. The results were:\n",
    "\n",
    "- R^2 of Random Forest model:\n",
    " 0.9961362204519756\n",
    "\n",
    "- MSE of Random Forest model:\n",
    " 0.2427981302570017\n",
    "\n",
    "- RMSE of Random Forest model:\n",
    " 0.4927455025233632\n",
    "\n",
    "- MAE of Random Forest model:\n",
    " 0.2427981302570017\n",
    "\n",
    "- 95% confidence interval of Random Forest model:\n",
    " [0.47478443 0.51007451]\n",
    "\n",
    "- Some train predictions:\n",
    " [ 4.695 17.595 12.815 29.825 14.495]\n",
    "\n",
    "- Some y_train:\n",
    " [4.5, 17.5, 13.0, 30.0, 14.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>distance/time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1546303600</td>\n",
       "      <td>1546304000</td>\n",
       "      <td>2.41</td>\n",
       "      <td>151</td>\n",
       "      <td>239</td>\n",
       "      <td>7.0</td>\n",
       "      <td>400</td>\n",
       "      <td>0.006025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546304387</td>\n",
       "      <td>1546305539</td>\n",
       "      <td>4.18</td>\n",
       "      <td>239</td>\n",
       "      <td>246</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1152</td>\n",
       "      <td>0.003628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1546302088</td>\n",
       "      <td>1546302517</td>\n",
       "      <td>2.09</td>\n",
       "      <td>163</td>\n",
       "      <td>229</td>\n",
       "      <td>6.5</td>\n",
       "      <td>429</td>\n",
       "      <td>0.004872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1546302721</td>\n",
       "      <td>1546303539</td>\n",
       "      <td>5.95</td>\n",
       "      <td>229</td>\n",
       "      <td>7</td>\n",
       "      <td>13.5</td>\n",
       "      <td>818</td>\n",
       "      <td>0.007274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1546304252</td>\n",
       "      <td>1546304972</td>\n",
       "      <td>3.38</td>\n",
       "      <td>141</td>\n",
       "      <td>234</td>\n",
       "      <td>10.0</td>\n",
       "      <td>720</td>\n",
       "      <td>0.004694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094954</th>\n",
       "      <td>1548975865</td>\n",
       "      <td>1548976629</td>\n",
       "      <td>6.68</td>\n",
       "      <td>186</td>\n",
       "      <td>13</td>\n",
       "      <td>14.5</td>\n",
       "      <td>764</td>\n",
       "      <td>0.008743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094955</th>\n",
       "      <td>1548977554</td>\n",
       "      <td>1548978100</td>\n",
       "      <td>2.16</td>\n",
       "      <td>68</td>\n",
       "      <td>233</td>\n",
       "      <td>8.0</td>\n",
       "      <td>546</td>\n",
       "      <td>0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094956</th>\n",
       "      <td>1548978468</td>\n",
       "      <td>1548979268</td>\n",
       "      <td>2.33</td>\n",
       "      <td>161</td>\n",
       "      <td>229</td>\n",
       "      <td>10.5</td>\n",
       "      <td>800</td>\n",
       "      <td>0.002913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094957</th>\n",
       "      <td>1548977514</td>\n",
       "      <td>1548978636</td>\n",
       "      <td>6.89</td>\n",
       "      <td>186</td>\n",
       "      <td>262</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1122</td>\n",
       "      <td>0.006141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094958</th>\n",
       "      <td>1548979056</td>\n",
       "      <td>1548980319</td>\n",
       "      <td>7.71</td>\n",
       "      <td>263</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1263</td>\n",
       "      <td>0.006105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7094807 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tpep_pickup_datetime  tpep_dropoff_datetime  trip_distance  \\\n",
       "0                  1546303600             1546304000           2.41   \n",
       "1                  1546304387             1546305539           4.18   \n",
       "2                  1546302088             1546302517           2.09   \n",
       "3                  1546302721             1546303539           5.95   \n",
       "4                  1546304252             1546304972           3.38   \n",
       "...                       ...                    ...            ...   \n",
       "7094954            1548975865             1548976629           6.68   \n",
       "7094955            1548977554             1548978100           2.16   \n",
       "7094956            1548978468             1548979268           2.33   \n",
       "7094957            1548977514             1548978636           6.89   \n",
       "7094958            1548979056             1548980319           7.71   \n",
       "\n",
       "         PULocationID  DOLocationID  fare_amount  trip_time  distance/time  \n",
       "0                 151           239          7.0        400       0.006025  \n",
       "1                 239           246         14.0       1152       0.003628  \n",
       "2                 163           229          6.5        429       0.004872  \n",
       "3                 229             7         13.5        818       0.007274  \n",
       "4                 141           234         10.0        720       0.004694  \n",
       "...               ...           ...          ...        ...            ...  \n",
       "7094954           186            13         14.5        764       0.008743  \n",
       "7094955            68           233          8.0        546       0.003956  \n",
       "7094956           161           229         10.5        800       0.002913  \n",
       "7094957           186           262         17.0       1122       0.006141  \n",
       "7094958           263             4         18.0       1263       0.006105  \n",
       "\n",
       "[7094807 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('D:\\\\Coding_data\\\\yellow_tripdata_2019-1_V2.csv')\n",
    "\n",
    "irrelevant_features = ['RatecodeID', 'payment_type']\n",
    "data.drop(irrelevant_features, inplace=True, axis=1)\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "data = data.replace([np.inf, -np.inf], 0).dropna(subset=data.columns, how=\"all\")\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>distance/time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tpep_pickup_datetime, tpep_dropoff_datetime, trip_distance, PULocationID, DOLocationID, fare_amount, trip_time, distance/time]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['trip_time'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7094807, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tpep_pickup_datetime     0\n",
       "tpep_dropoff_datetime    0\n",
       "trip_distance            0\n",
       "PULocationID             0\n",
       "DOLocationID             0\n",
       "fare_amount              0\n",
       "trip_time                0\n",
       "distance/time            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.notnull().values.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tpep_pickup_datetime     7094807\n",
       "tpep_dropoff_datetime    7094807\n",
       "trip_distance            7094807\n",
       "PULocationID             7094807\n",
       "DOLocationID             7094807\n",
       "fare_amount              7094807\n",
       "trip_time                7094807\n",
       "distance/time            7094807\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isfinite(data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tpep_pickup_datetime     False\n",
       "tpep_dropoff_datetime    False\n",
       "trip_distance            False\n",
       "PULocationID             False\n",
       "DOLocationID             False\n",
       "fare_amount              False\n",
       "trip_time                False\n",
       "distance/time            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tpep_pickup_datetime  tpep_dropoff_datetime  trip_distance  PULocationID  \\\n",
      "0            1546303600             1546304000           2.41           151   \n",
      "1            1546304387             1546305539           4.18           239   \n",
      "2            1546302088             1546302517           2.09           163   \n",
      "3            1546302721             1546303539           5.95           229   \n",
      "4            1546304252             1546304972           3.38           141   \n",
      "\n",
      "   DOLocationID  fare_amount  trip_time  distance/time  \n",
      "0           239          7.0        400       0.006025  \n",
      "1           246         14.0       1152       0.003628  \n",
      "2           229          6.5        429       0.004872  \n",
      "3             7         13.5        818       0.007274  \n",
      "4           234         10.0        720       0.004694  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data and calculating mean and scale of each feature in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = 'fare_amount'\n",
    "\n",
    "X = data.drop(predict, axis=1)\n",
    "y = data[predict]\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1330277, 7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I split the data into total of 40 sub files, just for practice how to do it with a very big data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    ny_dir = os.path.join(\"datasets\", \"ny_taxi\")\n",
    "    os.makedirs(ny_dir, exist_ok=True)\n",
    "    path_format = os.path.join(ny_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = data.columns\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the head of the first train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>distance/time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.546898e+09</td>\n",
       "      <td>1.546900e+09</td>\n",
       "      <td>7.56</td>\n",
       "      <td>161.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.547062e+09</td>\n",
       "      <td>1.547063e+09</td>\n",
       "      <td>1.13</td>\n",
       "      <td>246.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.547571e+09</td>\n",
       "      <td>1.547575e+09</td>\n",
       "      <td>17.64</td>\n",
       "      <td>48.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3837.0</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.546332e+09</td>\n",
       "      <td>1.546333e+09</td>\n",
       "      <td>5.63</td>\n",
       "      <td>239.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.546695e+09</td>\n",
       "      <td>1.546696e+09</td>\n",
       "      <td>1.45</td>\n",
       "      <td>113.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tpep_pickup_datetime  tpep_dropoff_datetime  trip_distance  PULocationID  \\\n",
       "0          1.546898e+09           1.546900e+09           7.56         161.0   \n",
       "1          1.547062e+09           1.547063e+09           1.13         246.0   \n",
       "2          1.547571e+09           1.547575e+09          17.64          48.0   \n",
       "3          1.546332e+09           1.546333e+09           5.63         239.0   \n",
       "4          1.546695e+09           1.546696e+09           1.45         113.0   \n",
       "\n",
       "   DOLocationID  fare_amount  trip_time  distance/time  \n",
       "0         231.0       1186.0   0.006374           17.5  \n",
       "1          68.0        217.0   0.005207            4.5  \n",
       "2          63.0       3837.0   0.004597           43.5  \n",
       "3          74.0        639.0   0.008811           12.0  \n",
       "4          79.0        404.0   0.003589            6.5  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here head of the first train file in text mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpep_pickup_datetime,tpep_dropoff_datetime,trip_distance,PULocationID,DOLocationID,fare_amount,trip_time,distance/time\n",
      "1546898373.0,1546899559.0,7.56,161.0,231.0,1186.0,0.006374367622259696,17.5\n",
      "1547062335.0,1547062552.0,1.13,246.0,68.0,217.0,0.0052073732718894,4.5\n",
      "1547570868.0,1547574705.0,17.64,48.0,63.0,3837.0,0.0045973416731821745,43.5\n",
      "1546332386.0,1546333025.0,5.63,239.0,74.0,639.0,0.008810641627543036,12.0\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\ny_taxi\\\\my_train_00.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_01.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_02.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_03.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_04.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_05.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_06.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_07.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_08.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_09.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_10.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_11.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_12.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_13.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_14.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_15.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_16.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_17.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_18.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\ny_taxi\\\\my_valid_00.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_01.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_02.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_03.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_04.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_05.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_06.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_07.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_08.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_valid_09.csv']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\ny_taxi\\\\my_test_00.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_01.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_02.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_03.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_04.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_05.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_06.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_07.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_08.csv',\n",
       " 'datasets\\\\ny_taxi\\\\my_test_09.csv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Building an Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\ny_taxi\\\\my_train_08.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can call the interleave() method to read from 5 files at a time and interleave their lines (skipping the first line of each file, which is the header row, using the skip() method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first rows (ignoring the header row) of 5 CSV files, chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'1547033768.0,1547034426.0,1.75,230.0,142.0,658.0,0.0026595744680851068,7.5'\n",
      "b'1547844428.0,1547844654.0,1.3,239.0,238.0,226.0,0.005752212389380531,4.5'\n",
      "b'1546625809.0,1546626707.0,4.51,211.0,256.0,898.0,0.0050222717149220495,13.0'\n",
      "b'1547369685.0,1547369824.0,0.64,142.0,142.0,139.0,0.00460431654676259,4.0'\n",
      "b'1548260637.0,1548261897.0,3.06,249.0,170.0,1260.0,0.002428571428571429,14.0'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 7 # X_train.shape[-1]\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
       " array([-0.86244404, -0.86244094,  1.2159067 , -0.9744824 , -1.0811594 ,\n",
       "         0.02102305,  0.11924608], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([19.5], dtype=float32)>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'1546324359.0,1546325409.0,9.38,100.0,87.0,1050.0,0.008933333333333335,19.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put all preproecessing stuff into one function, it will create and return a dataset that will efficiently load NY taxi data from multiple CSV files, then shuffle it, preprocess it and batch it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x00000259105504C8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x00000259105504C8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.8085261  -0.808847   -0.27938288 -1.7654948   0.0126943  -0.1040526\n",
      "   0.00435072]\n",
      " [-0.12686096 -0.12726529 -0.6363665  -0.320376   -0.28955474 -0.15032601\n",
      "  -0.04942088]\n",
      " [ 0.764566    0.7642394  -0.31306058 -0.7615175  -0.7933032  -0.11733903\n",
      "   0.01622542]], shape=(3, 7), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[8. ]\n",
      " [5.5]\n",
      " [7.5]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[ 0.75930375  0.7590581  -0.09527817  1.2920723  -0.01609133 -0.0799996\n",
      "   0.01786532]\n",
      " [-0.06792364 -0.0684901  -0.6880056   1.1247427  -0.16001944 -0.18674915\n",
      "   0.03348891]\n",
      " [ 0.07610039  0.07569532 -0.4230744   1.5050372   1.0777624  -0.13268714\n",
      "   0.00731695]], shape=(3, 7), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[9.5]\n",
      " [4. ]\n",
      " [6.5]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the csv_reader_dataset() function to create a dataset for the training set (ensuring it repeats the data forever), the validation set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x000002591092FDC8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x000002591092FDC8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x000002591092F828> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x000002591092F828> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x00000259108B88B8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function csv_reader_dataset.<locals>.<lambda> at 0x00000259108B88B8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 0\n",
    "\n",
    "Sequential MLP with one hidden layer of 30 perceptrons with Nadam optimizer and 10 epochs. First I used SGD optimizer but the loss was falling into NaN (but there are no Nans in the dataset).\n",
    "\n",
    "The evaluation on the test set resulted with a loss of 0.848."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 124713 steps\n",
      "Epoch 1/10\n",
      "124713/124713 [==============================] - 669s 5ms/step - loss: 15.1019 - val_loss: 369.7980\n",
      "Epoch 2/10\n",
      "124713/124713 [==============================] - 685s 5ms/step - loss: 14.9758 - val_loss: 379.9470\n",
      "Epoch 3/10\n",
      "124713/124713 [==============================] - 681s 5ms/step - loss: 16.1778 - val_loss: 382.8201\n",
      "Epoch 4/10\n",
      "124713/124713 [==============================] - 678s 5ms/step - loss: 15.1982 - val_loss: 402.0623\n",
      "Epoch 5/10\n",
      "124713/124713 [==============================] - 675s 5ms/step - loss: 11.5782 - val_loss: 233.8936\n",
      "Epoch 6/10\n",
      "124713/124713 [==============================] - 680s 5ms/step - loss: 10.4308 - val_loss: 292.3883\n",
      "Epoch 7/10\n",
      "124713/124713 [==============================] - 676s 5ms/step - loss: 9.5422 - val_loss: 200.6856\n",
      "Epoch 8/10\n",
      "124713/124713 [==============================] - 673s 5ms/step - loss: 8.1975 - val_loss: 165.2209\n",
      "Epoch 9/10\n",
      "124713/124713 [==============================] - 677s 5ms/step - loss: 8.1027 - val_loss: 207.1066\n",
      "Epoch 10/10\n",
      "124713/124713 [==============================] - 678s 5ms/step - loss: 8.5650 - val_loss: 231.6055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25910d5df08>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55428/55428 [==============================] - 128s 2ms/step - loss: 0.8483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8483280139496583"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1\n",
    "\n",
    "Like model 0 Sequential MLP with one hidden layer of 30 perceptrons with Nadam optimizer but with 30 epochs instead of 10.\n",
    "\n",
    "The evaluation on the test set resulted with a loss of 0.515 better than model 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"mse\", optimizer=keras.optimizers.Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 124713 steps\n",
      "Epoch 1/30\n",
      "124713/124713 [==============================] - 627s 5ms/step - loss: 31.5494 - val_loss: 858.6717\n",
      "Epoch 2/30\n",
      "124713/124713 [==============================] - 678s 5ms/step - loss: 33.1069 - val_loss: 878.9590\n",
      "Epoch 3/30\n",
      "124713/124713 [==============================] - 676s 5ms/step - loss: 34.3041 - val_loss: 925.6583\n",
      "Epoch 4/30\n",
      "124713/124713 [==============================] - 708s 6ms/step - loss: 32.4482 - val_loss: 776.8169\n",
      "Epoch 5/30\n",
      "124713/124713 [==============================] - 646s 5ms/step - loss: 28.5254 - val_loss: 726.7123\n",
      "Epoch 6/30\n",
      "124713/124713 [==============================] - 649s 5ms/step - loss: 28.3355 - val_loss: 707.9751\n",
      "Epoch 7/30\n",
      "124713/124713 [==============================] - 649s 5ms/step - loss: 26.3211 - val_loss: 682.2801\n",
      "Epoch 8/30\n",
      "124713/124713 [==============================] - 658s 5ms/step - loss: 24.3489 - val_loss: 569.8252\n",
      "Epoch 9/30\n",
      "124713/124713 [==============================] - 652s 5ms/step - loss: 22.6697 - val_loss: 519.8048\n",
      "Epoch 10/30\n",
      "124713/124713 [==============================] - 650s 5ms/step - loss: 21.5129 - val_loss: 494.6643\n",
      "Epoch 11/30\n",
      "124713/124713 [==============================] - 652s 5ms/step - loss: 17.9783 - val_loss: 477.7866\n",
      "Epoch 12/30\n",
      "124713/124713 [==============================] - 661s 5ms/step - loss: 17.2932 - val_loss: 462.8804\n",
      "Epoch 13/30\n",
      "124713/124713 [==============================] - 661s 5ms/step - loss: 18.7309 - val_loss: 443.4340\n",
      "Epoch 14/30\n",
      "124713/124713 [==============================] - 661s 5ms/step - loss: 16.7447 - val_loss: 413.5829\n",
      "Epoch 15/30\n",
      "124713/124713 [==============================] - 642s 5ms/step - loss: 15.8393 - val_loss: 391.0649\n",
      "Epoch 16/30\n",
      "124713/124713 [==============================] - 669s 5ms/step - loss: 15.5503 - val_loss: 387.5401\n",
      "Epoch 17/30\n",
      "124713/124713 [==============================] - 680s 5ms/step - loss: 16.0745 - val_loss: 333.3394\n",
      "Epoch 18/30\n",
      "124713/124713 [==============================] - 662s 5ms/step - loss: 15.1050 - val_loss: 304.7621\n",
      "Epoch 19/30\n",
      "124713/124713 [==============================] - 652s 5ms/step - loss: 13.7135 - val_loss: 352.0490\n",
      "Epoch 20/30\n",
      "124713/124713 [==============================] - 648s 5ms/step - loss: 12.4771 - val_loss: 342.9655\n",
      "Epoch 21/30\n",
      "124713/124713 [==============================] - 646s 5ms/step - loss: 13.7532 - val_loss: 277.3140\n",
      "Epoch 22/30\n",
      "124713/124713 [==============================] - 646s 5ms/step - loss: 14.3492 - val_loss: 335.7438\n",
      "Epoch 23/30\n",
      "124713/124713 [==============================] - 648s 5ms/step - loss: 13.1643 - val_loss: 263.7669\n",
      "Epoch 24/30\n",
      "124713/124713 [==============================] - 655s 5ms/step - loss: 13.7930 - val_loss: 279.3049\n",
      "Epoch 25/30\n",
      "124713/124713 [==============================] - 647s 5ms/step - loss: 12.5766 - val_loss: 332.9622\n",
      "Epoch 26/30\n",
      "124713/124713 [==============================] - 648s 5ms/step - loss: 12.7723 - val_loss: 281.3983\n",
      "Epoch 27/30\n",
      "124713/124713 [==============================] - 649s 5ms/step - loss: 12.1638 - val_loss: 239.0367\n",
      "Epoch 28/30\n",
      "124713/124713 [==============================] - 653s 5ms/step - loss: 12.3936 - val_loss: 314.7085\n",
      "Epoch 29/30\n",
      "124713/124713 [==============================] - 650s 5ms/step - loss: 13.6860 - val_loss: 267.7820\n",
      "Epoch 30/30\n",
      "124713/124713 [==============================] - 652s 5ms/step - loss: 13.7645 - val_loss: 268.5801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2599a884988>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model1.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=30,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55428/55428 [==============================] - 139s 3ms/step - loss: 0.5157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5156511533886443"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2\n",
    "\n",
    "Sequential MLP with two hidden layers of 50 and 20 perceptrons with Nadam optimizer with 10 epochs.\n",
    "\n",
    "The evaluation on the test set resulted with a loss of 0.244 better than model 0 and model 1.\n",
    "\n",
    "This model reached the MSE from Random Forest model I did before. So on the end, due to the very long computing time for MLP the Random Forest would be the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(20, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=\"mse\", optimizer=keras.optimizers.Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 124713 steps\n",
      "Epoch 1/10\n",
      "124713/124713 [==============================] - 641s 5ms/step - loss: 11.9638 - val_loss: 322.2403\n",
      "Epoch 2/10\n",
      "124713/124713 [==============================] - 719s 6ms/step - loss: 16.3589 - val_loss: 318.4167\n",
      "Epoch 3/10\n",
      "124713/124713 [==============================] - 698s 6ms/step - loss: 11.7473 - val_loss: 117.4863\n",
      "Epoch 4/10\n",
      "124713/124713 [==============================] - 691s 6ms/step - loss: 10.4907 - val_loss: 65.4218\n",
      "Epoch 5/10\n",
      "124713/124713 [==============================] - 716s 6ms/step - loss: 5.1954 - val_loss: 126.1546\n",
      "Epoch 6/10\n",
      "124713/124713 [==============================] - 722s 6ms/step - loss: 4.1273 - val_loss: 29.2995\n",
      "Epoch 7/10\n",
      "124713/124713 [==============================] - 706s 6ms/step - loss: 1.9250 - val_loss: 7.7053\n",
      "Epoch 8/10\n",
      "124713/124713 [==============================] - 697s 6ms/step - loss: 0.9552 - val_loss: 3.0105\n",
      "Epoch 9/10\n",
      "124713/124713 [==============================] - 702s 6ms/step - loss: 0.7922 - val_loss: 0.7374\n",
      "Epoch 10/10\n",
      "124713/124713 [==============================] - 722s 6ms/step - loss: 0.4341 - val_loss: 0.2336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x259ca63a408>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model2.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55428/55428 [==============================] - 152s 3ms/step - loss: 0.2445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2445206579807311"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
